

%\begin{rSection}{Research Description}
\section*{Research Description and Motivation}
Graph data is one of the most used data structures in computer science. Massive graphs are getting generated regularly by applications which maintain a relationship between different data entities, for example: the social network in social media, publications, and authors in DBLP, web pages and hyperlinks, sensor data, etc. Analyzing these graphs to compute the graph properties like connectivity, shortest path, node distance, etc. is a well know problem. However, traditional methods require multiple passes over the huge graphs. Hence analyzing them under the streaming model is becoming more and more popular. Unlike static graph mining techniques for large graphs, graph stream mining possesses a more complex challenge where the large graph is constantly updating and evolving over time by the addition of new edges and the continuous deletion of old edges. Calculating properties such as the ones mentioned above on the evolving graph is a very interesting research problem in graph data stream mining. For example finding the shortest path between all pairs of nodes in a graph is an interesting problem for large graphs like social media as it could be used to solve numerous graph analysis problems such as centrality computation, community detection and node separation. One of the approaches is to create synopses of the graphs to preserve these properties. These synopses are sparse sub graphs of the original graph which could be used to approximate the properties of the original graph. The size of these synopses is very small compared to the original graph and hence can be kept in memory for faster mining. Another approach to handle massive graphs are by using  distributed and parallel systems like Apache Spark GraphX, Graph Engine, Apache Giraph etc. to partition the graph data over different systems and manage the memory usage and computation time. These technique works on the principle of divide and conquers by dividing the computation and data over different machines. For dynamic graph data, however, doing a proper partitioning of data over different machines, to avoid too many communication, is much more complicated than for a data-set consisting of independent records or a static graph.

In my Ph.D. research study, we are using a combination of both approaches to solve some of the graph data stream mining problems. We first developed a novel incremental algorithm to maintain "Neighbourhood Profile" sketch of all the nodes in a graph using an extension of HyperLogLog sketch. Then we developed some variation of this sketch and used it for studying information flow in interaction network like micro-blog based social network twitter and also on location based social networks like FourSquare. Currently, I am working in developing a distributed version of our proposed algorithms in SPARK-GraphX and trying to address problems related to streaming graph and graph partitioning strategy to improve performance.
% We will first work on developing algorithms, which gives an approximate solution with high probability, for mining graph data streams that constantly evolve over time and then extend the developed solution further so that distributed systems and parallelization can be used to improve the performance.
%\end{rSection}

%\begin{rSection}{Motivation}
 %I am working on graph data mining and hence studying the current state of the art graph management systems which handles graph based data would be very useful for my PhD study. I am looking forward to learn and understand how different graph data sets are handled and which properties of graph data is more useful to calculate. Understanding the current offline systems and its challenges with respect to large data will help me in my research on how to solve these problems in a more online (streaming) setups. 
%\end{rSection}
